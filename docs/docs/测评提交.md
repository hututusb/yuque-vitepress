
#### å¤§èµ›æŠ¥åå§“åï¼šcomefly
#### é˜¿é‡Œäº‘è´¦å·ï¼šaliyun4051250644

#### å…·ä½“æ“ä½œ

- æ“ä½œæ­¥éª¤

        1.é€‰æ‹©å…è´¹èµ„æº

- è½¯ä»¶å®‰è£…é…ç½®

        1.å®‰è£…anolis-epao-releaseåŒ…ï¼Œå†å®‰è£…ä¾èµ–ï¼Œæ›´æ–°pipï¼Œå®‰è£… git lfsä»¥ä¾¿äºå¤§æ–‡ä»¶ä¼ è¾“
```
yum install -y anolis-epao-release
```
```
yum install -y git git-lfs wget curl gcc gcc-c++ tar unzip pytorch gperftools-libs
```
```
python -m pip install --upgrade pip
```
```
git lfs install
```

- ä¸‹è½½æ¨¡å‹ï¼Œéƒ¨ç½²

          1.å› ä¸ºå·²ç»å®‰è£…å¥½äº†gpt-2é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ‰€ä»¥æš‚æ—¶ä¸ç”¨ä¸‹è½½

1. éƒ¨ç½²è¿è¡Œç¯å¢ƒ
```
pip install --ignore-installed pyyaml==5.1
pip install transformers streamlit intel-openmp
```
3.æ‰§è¡Œå‘½ä»¤åˆ›å»ºæ–‡ä»¶å¤¹
```
mkdir write-with-transformer && cd write-with-transformer
```
4.è¿è¡Œ
```
vim app.py
```
5.ç²˜è´´ä»£ç 
```
import streamlit as st
from transformers import AutoTokenizer, AutoModelWithLMHead

tokenizer = AutoTokenizer.from_pretrained("../gpt2-large")
model = AutoModelWithLMHead.from_pretrained("../gpt2-large")

def infer(input_ids, max_length, temperature, top_k, top_p):
    output_sequences = model.generate(input_ids=input_ids,max_length=max_length,temperature=temperature,
        top_k=top_k,top_p=top_p,do_sample=True,num_return_sequences=1)
    return output_sequences

default_value = "Hello, I'm a language model,"

st.title("Write with Transformers ğŸ¦„")
sent = st.text_area("Text", default_value, height = 275)
max_length = st.sidebar.slider("Max Length", min_value = 10, max_value=30)
temperature = st.sidebar.slider("Temperature", value = 1.0, min_value = 0.0, max_value=1.0, step=0.05)
top_k = st.sidebar.slider("Top-k", min_value = 0, max_value=5, value = 0)
top_p = st.sidebar.slider("Top-p", min_value = 0.0, max_value=1.0, step = 0.05, value = 0.9)

encoded_prompt = tokenizer.encode(sent, add_special_tokens=False, return_tensors="pt")
if encoded_prompt.size()[-1] == 0:
    input_ids = None
else:
    input_ids = encoded_prompt

output_sequences = infer(input_ids, max_length, temperature, top_k, top_p)

for generated_sequence_idx, generated_sequence in enumerate(output_sequences):
    print(f"=== GENERATED SEQUENCE {generated_sequence_idx + 1} ===")
    generated_sequences = generated_sequence.tolist()
    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)
    total_sequence = (
        sent + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]
    )
    generated_sequences.append(total_sequence)
    print(total_sequence)

st.write(generated_sequences[-1])
```
```
export OMP_NUM_THREADS=$(nproc --all)
export KMP_AFFINITY=granularity=fine,compact,1,0
export LD_PRELOAD=/usr/lib64/libtcmalloc.so.4:/usr/local/lib/libiomp5.so
streamlit run app.py --server.port 7860
```
#### å®éªŒæ“ä½œå®Œæˆæˆªå›¾
![](../images/02036e27815a3834f0d59863499078ea.png)
![](../images/bbc080c91383e41608b7e699c79d31b6.png)
#### å¯¹å®éªŒçš„åé¦ˆæˆ–å»ºè®®

- æ¨¡å‹æ€§èƒ½è¿‡äºå¼±å°ï¼Œé€‚é‡å¢åŠ æœåŠ¡å™¨é…ç½®ï¼Œæœ€å¥½æ˜¯GPUæœåŠ¡ï¼Œ
- å¢åŠ å®éªŒç¯èŠ‚ï¼Œ


